# -*- coding: utf-8 -*-
"""Dibimbing - Tugas 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/dibimbing-tugas-1-a637a8f8-8c8a-4a30-86e8-d17b2c3bac7c.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240829/auto/storage/goog4_request%26X-Goog-Date%3D20240829T020038Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D98535fbc026e1f84a7b469f6cfcadf779c8dc6420aa15812545b9438dd7f0790667ad5a43edec3470496355c4d10782840de588d4ae6e46320080f61ab001b92e8bd011f04f00a765c80610866a818d26d69f366998ea652ec2267af2c64b18c0888f4dedbd84e2be002b4ba48390c3a949555d7630dcb64de4dda85c66f9e282327386d4473645cf955d1d98febbb8521e38a1d2458c456d7c6a6717346d49a433616e251e9b26a24bda11607d59cf716f7e04e6b4e959f00d5b89982406cc1c4397a37c506b8eb50e16bef003a7cd50583995166113f428970a496926931a01e477f324477407c2a0747707aa5dd8773fff51f853e34de209c4ea2e759c7be
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'titanic:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F3136%2F26502%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240829%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240829T020038Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9805b0e7abcfcdaf1bb65e492fa31b3019925f2a91ef54d092fddd4cf8b104f63c99450de542010c0caf13b6a1030a3af16d3fdd6f9a32ea9f6f0563d91472c1535e294b582ab2c26b9c78a42a4e87cc1220db5353e1ade8a1f02db5b84786f73d6497c6300376b9dccf99f6a67f560c05c6775cbea849d4f1b2b0a2a268f5cca2f74959a27c256439f8508006f7ba61d0ce46447709a19296aa5830202bc8a76f335f118833687dc06c81d50be284e8863d5149fc4650d66e2ab986c775db714d9a993858211b551e26082c9c20899cdcf6695a8a0aa5d93e8669dcd9dbf4133c7e0b1bed251e278795ca98653eae6df0812c24170e7e5a119968d467a60dc6'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

"""# Input Data"""

df = pd.read_csv('/kaggle/input/titanic/train.csv')
df[100:105]

df['Age'] = df['Age'].fillna(df['Age'].mean())
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])
df = df.drop('Cabin', axis=1)

df.isnull().sum()

"""# Feature Engineering

## Label Encoding
"""

le = LabelEncoder()

df['Sex'] = le.fit_transform(df['Sex'])
df['Embarked'] = le.fit_transform(df['Embarked'])

df.head()

"""## Features Selection"""

# features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']

X = df.drop(['Survived', 'Ticket', 'Name'], axis=1)
y = df['Survived']

"""# Train Test Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

X_train.head

"""# Modeling"""

model = RandomForestClassifier(n_estimators=1000, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")

"""## Get The Best Value For Number of Estimators"""

n_estimators = [n for n in range(190, 1000 + 1, 10)]

for estimator in n_estimators:
    model = RandomForestClassifier(n_estimators=estimator, random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)

    print(f"n_estimator = {estimator}, accuracy: {accuracy:.2f}")

n_estimator = 220

model = RandomForestClassifier(n_estimators=n_estimator, random_state=42, verbose=1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print(f"n_estimator = {n_estimator}, accuracy: {accuracy:.2f}")

"""# Predict Model"""

comparison_df = pd.DataFrame({'Actual': y_test,
                             'Predicted': y_pred})

comparison_df.head()

"""# Confusion Matrix"""

conf_mtrx = confusion_matrix(y_test, y_pred)
print('Confusion Matrix')
print(conf_mtrx)

sns.heatmap(conf_mtrx, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# Features Importance"""

all_features = X_train.columns
features_importance = model.feature_importances_

importances_df = pd.DataFrame({'Features': all_features,
                               'Importances': features_importance})

importances_df = importances_df.sort_values('Importances', ascending=False)
importances_df